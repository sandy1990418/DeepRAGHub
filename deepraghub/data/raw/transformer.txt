Transformer is a deep learning model introduced in 2017, used primarily in the field of natural language processing (NLP). Unlike previous sequence-to-sequence models, Transformer does not employ recurrence and instead relies entirely on an attention mechanism to draw global dependencies between input and output. The Transformer allows for significantly more parallelization and can reach a new state of the art in translation quality.